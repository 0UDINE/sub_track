---

# ğŸ“Š Subscription Data Pipeline  

**A data engineering pipeline demonstrating batch-to-stream processing** using Python, SQL Server, Apache Kafka, and JSON intermediates. Generates synthetic subscription data, processes it through staged ETL, and delivers analytics-ready results.

---

## ğŸ—ï¸ Architecture Overview  

![Architecture Diagram](assets/images/architecture_diagram.png)  

### **Data Flow**  
```mermaid
flowchart LR
    Faker -->|"Batch save to JSON (raw)"| JSON_Raw
    JSON_Raw -->|"Publish to Kafka topic"| Kafka
    Kafka -->|"Load to SQL (raw)"| SQL_Raw
    SQL_Raw -->|"ETL Processing"| SubTrack
    SubTrack -->|"Save to JSON (processed)"| JSON_Processed
    JSON_Processed -->|"Publish to Kafka"| Kafka
    Kafka -->|"Load to SQL (processed)"| SQL_Processed
```

### **Key Stages**  

1. **Generation & Raw JSON Stage**  
   - **Faker** generates synthetic data â†’ persisted as **raw JSON files** (for validation/replay)  
   - **Kafka Producer** streams raw JSON records to `subscriptions_raw` topic  

2. **Raw Data Pipeline**  
   - **Kafka Consumer** ingests raw JSON into SQL Server (`raw_subscriptions`)  
   - **SubTrack** transforms data â†’ outputs **processed JSON files**  

3. **Processed Data Pipeline**  
   - **Kafka Producer** streams processed JSON to downstream topics  
   - Final data lands in SQL Server (`processed_subscriptions`)  

---

## ğŸ› ï¸ Technology Stack  

| Component           | Tools                                                                 |
|---------------------|-----------------------------------------------------------------------|
| **Data Generation** | Python Faker, JSON intermediates                                      |
| **Streaming**       | Apache Kafka (raw/processed topics)                                   |
| **Database**        | SQL Server (`raw_subscriptions`, `processed_subscriptions`)           |
| **Processing**      | Pandas (ETL), SQLAlchemy (ORM)                                       |
| **Connectivity**    | PyODBC (SQL Server), confluent-kafka (Python client)                 |

---

## ğŸš€ Quick Start  

### **1. Setup**  
```bash
git clone <repo-url> && cd subscription-pipeline
python -m venv venv && source venv/bin/activate  # or .\venv\Scripts\activate
pip install -r requirements.txt
```

### **2. Configure**  
- **SQL Server**: Update `database/config.py`  
- **Kafka**: Modify topics in `kafka/config.py`  

### **3. Run Pipeline**  
```bash
# Terminal 1: Generate data â†’ raw JSON
python data_generation/faker_generator.py  

# Terminal 2: Stream raw JSON to Kafka
python kafka/raw_producer.py  

# Terminal 3: Process data â†’ publish processed JSON
python processing/transformer.py  

# Terminal 4: Load processed JSON to DB
python kafka/processed_consumer.py  
```

---

## ğŸ“ Project Structure  

```
subscription_pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Raw JSON from Faker
â”‚   â””â”€â”€ processed/            # Transformed JSON
â”œâ”€â”€ data_generation/
â”‚   â””â”€â”€ faker_generator.py    # Generates â†’ data/raw/*.json
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ raw_producer.py       # Streams raw JSON â†’ Kafka
â”‚   â””â”€â”€ processed_consumer.py # Loads processed JSON â†’ DB
â””â”€â”€ processing/
    â””â”€â”€ transformer.py        # SQL â†’ Transform â†’ data/processed/
```

---

## ğŸ”„ JSON Integration Points  

1. **Raw Data Persistence**  
   - Faker outputs to `data/raw/subscriptions_<timestamp>.json`  
   - Optional: Replay JSON files for debugging by re-running producers  

2. **Processed Data Handoff**  
   - SubTrack saves cleansed data to `data/processed/subscriptions_<batch>.json`  
   - Kafka streams these files to final DB tables  

---

## ğŸ’¡ Why JSON?  
- **Development**: Inspect/intermediate data without DB dependencies  
- **Testing**: Replay specific batches from JSON files  
- **Portability**: Decouple generation from streaming logic  

---

## ğŸ› Troubleshooting  

**Issue** | **Solution**  
---|---
Kafka not streaming | Verify ZooKeeper/Kafka are running (`jps`)  
JSON files not found | Check `data/raw/` and `data/processed/` paths  
DB connection fails | Validate `pyodbc` connection string in `config.py`  

---

## ğŸ“œ License  
MIT Â© [Oussama Elalouaoui]  

